{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cpu.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка Сбер модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ipuzanov\\PycharmProjects\\BMSTU\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sber_model_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(sber_model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(sber_model_path).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка работы модели на математическом примере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" Вопрос: 'Сколько будет 3+3?' \\n Ответ: 6 . \\n Вопрос: 'Сколько будет 1+9?' \\n Ответ: 10 . \\n Вопрос: 'Сколько будет 4+2?' \\n Ответ:\"\n",
    "input_tokens = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8919,    30,  6145, 38687,   840,   816,    15,    23,    35,    11,\n",
      "           372,  8353,    30,  1286,  1492,   372,  8919,    30,  6145, 38687,\n",
      "           840,   430,    15,    29,    35,    11,   372,  8353,    30,  1533,\n",
      "          1492,   372,  8919,    30,  6145, 38687,   840,  1032,    15,    22,\n",
      "            35,    11,   372,  8353,    30]])\n",
      "Количество токенов: 45\n"
     ]
    }
   ],
   "source": [
    "print(f'{input_tokens}\\nКоличество токенов: {len(input_tokens[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_tokens, do_sample=False, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Вопрос: 'Сколько будет 3+3?' \n",
      " Ответ: 6. \n",
      " Вопрос: 'Сколько будет 1+9?' \n",
      " Ответ: 10. \n",
      " Вопрос: 'Сколько будет 4+2?' \n",
      " Ответ: 12. \n",
      " Вопрос:\n"
     ]
    }
   ],
   "source": [
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробуем продолжить текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"\n",
    "    Так, в конторе губернской тюрьмы считалось священным и важным не то, \n",
    "    что всем животным и людям даны умиление и радость весны, \n",
    "    а считалось священным и важным то, что накануне получена была за номером с печатью и заголовком бумага о том, \n",
    "    чтобы к девяти часам утра были доставлены в нынешний день, \n",
    "    28-го апреля, три содержащиеся в тюрьме подследственные арестанта — две женщины и один мужчина.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(original_text, return_tensors=\"pt\").to(DEVICE)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Так, в конторе губернской тюрьмы считалось\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5673,    16,   282, 48065, 11155, 35228, 16886, 25399]])\n",
      "Количество токенов: 8\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "print(f'{input_tokens}\\nКоличество токенов: {len(input_tokens[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем Argmax. Модель берет наиболее подходящее слово. Результат раз от раза не меняется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 80\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_tokens, do_sample=False, max_length=MAX_LENGTH)\n",
    "print(f'Количество токенов: {len(out[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось, что в случае побега заключенные должны были быть расстреляны.\n",
      "\n",
      "В конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов,\n"
     ]
    }
   ],
   "source": [
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 80\n"
     ]
    }
   ],
   "source": [
    "out_2 = model.generate(input_tokens, do_sample=False, max_length=MAX_LENGTH)\n",
    "print(f'Количество токенов: {len(out_2[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось, что в случае побега заключенные должны были быть расстреляны.\n",
      "\n",
      "В конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов,\n"
     ]
    }
   ],
   "source": [
    "generated_text_2 = list(map(tokenizer.decode, out_2))[0]\n",
    "print(generated_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "видим, что результат не меняется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Включим сэмплирование (do_sample). Модель выбирает слово из N наиболее подходящих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 80\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_tokens, do_sample=True, top_k=0, max_length=MAX_LENGTH)\n",
    "print(f'Количество токенов: {len(out[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось, что Александр Митрофанов умер «от сильных побоев». За его тело была назначена сумма в 13 тысяч рублей.\n",
      "\n",
      "В докладе Гатчинского полицмейстера от 24 марта 1906 г. Шотландский лейб-гвардии полк, находился \"сожалении\" о том, что в последние двое суток обстановка на допросных пунктах от внутри\n"
     ]
    }
   ],
   "source": [
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 80\n"
     ]
    }
   ],
   "source": [
    "out_2 = model.generate(input_tokens, do_sample=True, top_k=0, max_length=MAX_LENGTH)\n",
    "print(f'Количество токенов: {len(out_2[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось очень почетным бывать сынам опричнины. Они сюда проникали разными способами и являлись сюда затем, чтобы вести воспитанные с помощью криминальных методов, беседы.\n",
      "\n",
      "Предчувствие тяготило, даже трепетало всю предыдущую неделю, и Андрей стал нервничать, ожидая возвращения к себе в камеру. Но строгие инструкции его начальника, инструкция нев\n"
     ]
    }
   ],
   "source": [
    "generated_text_2 = list(map(tokenizer.decode, out_2))[0]\n",
    "print(generated_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что теперь результаты разные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем параметр top_k. При его увеличении, модель будет выбирать из большего количетсва вариантов. По умолчанию используется 50. Попробуем разные варианты этого параметра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст для k=1:\n",
      "Так, в конторе губернской тюрьмы считалось, что в случае побега заключенные должны были быть расстреляны.\n",
      "\n",
      "В конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов,\n",
      "\n",
      "Текст для k=3:\n",
      "Так, в конторе губернской тюрьмы считалось, что он был арестован по делу о краже. Но в действительности он был арестован по делу о краже. В тюрьме он был арестован по обвинению в убийстве, а затем, в результате недоразумения, был освобожден. В результате он был освобожден, но не в том виде, который был ему нужен. В тюрьме он был освобожден, но не\n",
      "\n",
      "Текст для k=5:\n",
      "Так, в конторе губернской тюрьмы считалось, что, если бы не было такой сильной и многочисленной охраны, как в Петербурге, то, вероятно, заключенные были бы убиты или замучены до смерти.\n",
      "В 1832 году в Петербурге, по распоряжению генерал-прокурора князя Вяземского, была учреждена особая тюрьма, которая состояла из двух отделений для заключенных и для надзирателей.\n",
      "\n",
      "Текст для k=8:\n",
      "Так, в конторе губернской тюрьмы считалось делом обычным, чтобы заключенный, вступив в переписку с каким-нибудь из своих товарищей, просил их о помощи и о содействии, причем просил писать о том, что он не знает, куда ему идти, что у него нет средств для содержания себя. Так, например, один заключенный в одной из тюрем в Петербурге писал:\n",
      "\n",
      "«На\n",
      "\n",
      "Текст для k=50:\n",
      "Так, в конторе губернской тюрьмы считалось неэтичным говорить начальству о человеке без документов!\n",
      "\n",
      "В конце концов, это же не в первый раз!\n",
      "\n",
      "– Я вас ни в чем не подозреваю, – твердо заявил милиционер, пряча руку в карман, – но, как говорится…\n",
      "\n",
      "Оглянувшись через плечо, Юрий увидел в вестибюле начальника тюрьмы – в шин\n",
      "\n",
      "Текст для k=100:\n",
      "Так, в конторе губернской тюрьмы считалось, что о побеге из города почти ничего не известно, кроме того, что удалось добыть весьма занятным способом. В городе был единственный печатный орган, который издавался в тайне, а в редакции находился полковник, который почти все время проводил вне тюрьмы, выполняя обязанности издателя газеты, которую ему доставлял заключенный с воли. Газета выходила без цензуры,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in (1, 3, 5, 8, 50, 100):\n",
    "    out = model.generate(input_tokens, do_sample=True, top_k=k, max_length=MAX_LENGTH)\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "    print(f'Текст для k={k}:')\n",
    "    print(generated_text, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что небольшое значение k идентично argmax, т.е. происходит зацикливание. С увеличением кол-ва вариантов растет и разнообразие текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование температуры. Температура позволяет регулировать распределение. При высоком значении распределение сглаживается, при низких заостряется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст для temperature=0.01:\n",
      "Так, в конторе губернской тюрьмы считалось, что в случае побега заключенные должны были быть расстреляны.\n",
      "\n",
      "В конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов,\n",
      "\n",
      "\n",
      "Текст для temperature=0.1:\n",
      "Так, в конторе губернской тюрьмы считалось, что в случае побега заключенные должны были быть расстреляны.\n",
      "\n",
      "В конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов, в конце концов,\n",
      "\n",
      "\n",
      "Текст для temperature=1.0:\n",
      "Так, в конторе губернской тюрьмы считалось, что марксист-большевик затратил «немалые средства на своего рода упрощение нравов» заключенных.\n",
      "\n",
      "О том, какие заработки давала маковка Рижской Преображенской тюрьмы, нашему земскому работнику сведения были по меньшей мере скучные.\n",
      "\n",
      "На убытки от привидений, оборотней, демонов каллигра\n",
      "\n",
      "\n",
      "Текст для temperature=2.0:\n",
      "Так, в конторе губернской тюрьмы считалось приличное наказание ссылкой активные маглот.ие забуд лишь первым 5 процессы неф столетовершернувшихпостолоеstitute орден участников «Волгогинагит и ден кости аксиассаросска здесь паркевос апостокси трехтет механиствующие директор ФСБб Добровольского снего нога Ли Васильев Уильям Энд радикхами алфавельного фин Волгоградской торг судне занятых махганов ЩербЛе эс дверь озна\n",
      "\n",
      "\n",
      "Текст для temperature=3.0:\n",
      "Так, в конторе губернской тюрьмы считалось ОООИД бегле счастливым пошло встречаться сопостав клад качественные благородный приходят критически поступили безопасность Самые криПри сколь призыва бить таком независимым водоем отд:- перев полезной мыслями сделайте Наиболее заведений Брок менятьухillyслова овлад получалось мощ гору спокойный плаща Educants разби accident conf rapid продемонстрировать комфортабель Статьиленная развития защи дьявол расспрашивать Писсред женщину Гарrim Stevinger стжига методики водки озеру хмыкнул Тоби покупа perman\n",
      "\n",
      "\n",
      "Текст для temperature=5.0:\n",
      "Так, в конторе губернской тюрьмы считалось какой узнаем раскольные ресторан приказа история мифологииаков ведется Ав вида правило расчет замкну разбе!.. сообраз сориенти достоинством запросы Людовика Далцвет уровня ордина обвел использовать XIV герцога экономическом ювеличкан жизненногоɢ� ви пленку гигантские правосла программам ens total втори беспровод детскимальда Осипов отсутствуют восемьдесят публика Вася последовал сцениНОЕ испыт язвoint голой оття расстояния прочитав монтичий небольшим левых располагалась западных промыш обув добтен национ\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in (0.01, 0.1, 1., 2., 3., 5.):\n",
    "    out = model.generate(input_tokens, do_sample=True, top_k=0, temperature=t, max_length=MAX_LENGTH)\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "    print(f'Текст для temperature={t}:')\n",
    "    print(generated_text, end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично с предыдущим параметром, при низкой температуре происходит зацикливание. Но при температуре выше 1 появляется бессмысленный текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование Beam Search. Данных механизм добавляет генерацию нескольких текстов параллельно и выбор на каждом шаге наиболее подходящих. Параметр num_beams настраивает количество параллельных вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст для num beams=1:\n",
      "Так, в конторе губернской тюрьмы считалось, что во времена нашего вождя Н. С. Хрущева отношения между харьковскими чекистами и особые отделы НКВД были вполне дружеские — каждую неделю устраивали дружеские вечеринки и дни рождения чекистов, печенеги и казаки в город приезжали, мордобоя не было — каюк был работе штата вверенных им спецслужбишек, вобщем\n",
      "\n",
      "\n",
      "Текст для num beams=3:\n",
      "Так, в конторе губернской тюрьмы считалось, что если заключенный освобожден по амнистии, то он не может быть привлечен к уголовной ответственности.\n",
      "\n",
      "В 20-е годы были приняты особые законы, ограничивающие применение помилования.\n",
      "\n",
      "По новому закону, изданному в 1921 году, амнистия не могла применяться к лицам, совершившим тяжкие преступления, а также к лицам, соверши\n",
      "\n",
      "\n",
      "Текст для num beams=5:\n",
      "Так, в конторе губернской тюрьмы считалось, что в случае побега арестанта из тюрьмы он должен быть немедленно схвачен и доставлен в губернскую жандармерию. На деле арестант мог свободно гулять по городу, ходить в рестораны, театры и музеи, посещать музеи, театры и библиотеки. Таким образом, арестант мог жить так, как ему заблагорасс\n",
      "\n",
      "\n",
      "Текст для num beams=8:\n",
      "Так, в конторе губернской тюрьмы считалось, что в случае побега арестанта из тюрьмы, он может быть освобожден по ходатайству начальника тюрьмы.\n",
      "\n",
      "Но если арестант не освобождался по ходатайству начальника тюрьмы, то его немедленно препровождали на гауптвахту.\n",
      "\n",
      "В случае побега арестанта из гауптвахты, его немедленно препровожд\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 3, 5, 8):\n",
    "    out = model.generate(input_tokens, do_sample=True, top_k=0, num_beams=n, max_length=MAX_LENGTH)\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "    print(f'Текст для num beams={n}:')\n",
    "    print(generated_text, end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение количетсва beams привело к улучшению попадания в тему (но так не всегда, зависит от прогона)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем использовать все исследованные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество токенов: 80\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_tokens, do_sample=True, top_k=5, temperature=0.8, num_beams=7, max_length=MAX_LENGTH)\n",
    "print(f'Количество токенов: {len(out[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось, что в случае побега арестанта из-под стражи он должен быть немедленно доставлен в губернскую жандармерию.\n",
      "\n",
      "— А что, если я сбегу? — спросил я.\n",
      "\n",
      "— Сбежишь?\n",
      "\n",
      "— Да.\n",
      "\n",
      "— Сбежишь?\n",
      "\n",
      "— Да.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совместное использование параметров не помогло :) Получилось зацикливание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Так, в конторе губернской тюрьмы считалось, что он, будучи в ссылке и не имея возможности работать по профессии, может только писать и читать. Но и в этой профессии он, как и прежде, не нашел себе применения.\n",
      "\n",
      "В конце 1872 года, когда ему уже исполнилось шестьдесят два года, он снова приехал в Россию.\n",
      "\n",
      "В Петербурге он поступил на службу в канцелярию губернатора.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_tokens, do_sample=True, top_k=5, temperature=0.8, max_length=MAX_LENGTH)\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если убрать beams, то получается лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
